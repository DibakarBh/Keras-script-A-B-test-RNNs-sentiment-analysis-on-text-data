{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dP-BJX4jbhgo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pathlib\n",
        "import shutil\n",
        "import random\n",
        "import string\n",
        "\n",
        "# --- Helper function to suppress TensorFlow C-level logs ---\n",
        "# 0 = all logs (default), 1 = filter INFO, 2 = filter WARNING, 3 = filter ERROR\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# === 1. Download and Prepare Data ===\n",
        "\n",
        "def download_and_extract_imdb():\n",
        "    \"\"\"\n",
        "    Downloads and extracts the ACL IMDB dataset.\n",
        "    Returns the pathlib.Path to the base extracted directory, or None on failure.\n",
        "    \"\"\"\n",
        "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    dataset_tar_gz = \"aclImdb_v1.tar.gz\"\n",
        "    target_imdb_dir_name = \"aclImdb\"\n",
        "    target_imdb_dir_path = pathlib.Path(target_imdb_dir_name)\n",
        "\n",
        "    # --- Cleanup previous extractions/downloads to ensure a clean slate ---\n",
        "    print(\"Cleaning up previous IMDB related files/directories if they exist...\")\n",
        "    if os.path.exists(target_imdb_dir_path):\n",
        "        print(f\"Removing existing '{target_imdb_dir_path}' directory...\")\n",
        "        shutil.rmtree(target_imdb_dir_path)\n",
        "    if os.path.exists(dataset_tar_gz):\n",
        "        print(f\"Removing existing '{dataset_tar_gz}' file...\")\n",
        "        os.remove(dataset_tar_gz)\n",
        "    # A non-standard extracted dir name from user's previous log\n",
        "    if os.path.exists(\"aclImdb_v1_extracted\"):\n",
        "        print(\"Removing existing 'aclImdb_v1_extracted' directory...\")\n",
        "        shutil.rmtree(\"aclImdb_v1_extracted\")\n",
        "    print(\"Cleanup complete.\")\n",
        "    # End Cleanup\n",
        "\n",
        "    print(\"Downloading IMDB dataset...\")\n",
        "    # cache_subdir='' places extraction in the current directory\n",
        "    # This utility returns the path to the *extracted directory*\n",
        "    extracted_dataset_path_str = keras.utils.get_file(\n",
        "        dataset_tar_gz, url,\n",
        "        untar=True, cache_dir='.',\n",
        "        cache_subdir='')\n",
        "\n",
        "    # Use the path returned by the keras utility\n",
        "    dataset_dir = pathlib.Path(extracted_dataset_path_str)\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Error: Keras utility did not extract to '{dataset_dir}'.\")\n",
        "        return None # Indicate failure\n",
        "\n",
        "    # dataset_dir is likely 'aclImdb_v1_extracted'. We want it to be 'aclImdb'.\n",
        "    if dataset_dir.name != target_imdb_dir_name:\n",
        "        print(f\"Renaming extracted directory from '{dataset_dir.name}' to '{target_imdb_dir_name}'...\")\n",
        "        if os.path.exists(target_imdb_dir_path):\n",
        "            shutil.rmtree(target_imdb_dir_path) # Remove 'aclImdb' if it somehow exists\n",
        "        shutil.move(str(dataset_dir), str(target_imdb_dir_path))\n",
        "        dataset_dir = target_imdb_dir_path # Update our path to point to 'aclImdb'\n",
        "\n",
        "    if not os.path.exists(dataset_dir):\n",
        "         print(f\"Error: Renamed directory '{dataset_dir}' not found.\")\n",
        "         return None\n",
        "\n",
        "    # Handle nested 'aclImdb/aclImdb' directory\n",
        "    nested_imdb_dir = dataset_dir / target_imdb_dir_name\n",
        "    if os.path.exists(nested_imdb_dir) and nested_imdb_dir.is_dir():\n",
        "        print(f\"Detected nested directory structure. Using '{nested_imdb_dir}'.\")\n",
        "        dataset_dir = nested_imdb_dir\n",
        "\n",
        "    # Remove the unsupervised training data\n",
        "    unsup_dir = dataset_dir / \"train\" / \"unsup\"\n",
        "    if os.path.exists(unsup_dir):\n",
        "        print(f\"Removing {unsup_dir}...\")\n",
        "        shutil.rmtree(unsup_dir)\n",
        "\n",
        "    return dataset_dir # Return the actual base aclImdb path\n",
        "\n",
        "def setup_custom_directories(base_imdb_dir, train_samples_count, val_samples_count):\n",
        "    \"\"\"\n",
        "    Sets up specific train/val/test directories based on assignment specs.\n",
        "    - train_dir_subset: Contains `train_samples_count` samples.\n",
        "    - val_dir: Contains `val_samples_count` samples. (SKIPS if exists)\n",
        "    - test_dir_main: Contains the remaining test samples. (SKIPS if exists)\n",
        "    \"\"\"\n",
        "    print(f\"\\nSetting up custom data directories (Train: {train_samples_count})...\")\n",
        "    main_train_dir = base_imdb_dir / \"train\"\n",
        "    main_test_dir = base_imdb_dir / \"test\"\n",
        "\n",
        "    # --- Create Validation Set (10,000 samples) ---\n",
        "    val_dir = base_imdb_dir / \"val_set\"\n",
        "    test_dir_main = base_imdb_dir / \"test_set\" # The remaining test files\n",
        "\n",
        "    # Only create val_set and test_set if they don't already exist.\n",
        "    if not os.path.exists(val_dir) or not os.path.exists(test_dir_main):\n",
        "        print(\"Creating new Validation and Test sets...\")\n",
        "        # Ensure directories are clean\n",
        "        if os.path.exists(val_dir): shutil.rmtree(val_dir)\n",
        "        if os.path.exists(test_dir_main): shutil.rmtree(test_dir_main)\n",
        "\n",
        "        os.makedirs(val_dir / \"pos\")\n",
        "        os.makedirs(val_dir / \"neg\")\n",
        "        os.makedirs(test_dir_main / \"pos\")\n",
        "        os.makedirs(test_dir_main / \"neg\")\n",
        "\n",
        "        val_per_cat = val_samples_count // 2\n",
        "\n",
        "        # Move 5,000 pos and 5,000 neg from test -> val\n",
        "        for category in (\"pos\", \"neg\"):\n",
        "            test_files = os.listdir(main_test_dir / category)\n",
        "            random.shuffle(test_files)\n",
        "            # Copy to validation\n",
        "            for fname in test_files[:val_per_cat]:\n",
        "                shutil.copy(main_test_dir / category / fname, val_dir / category / fname)\n",
        "            # Copy remaining to new test set\n",
        "            for fname in test_files[val_per_cat:]:\n",
        "                shutil.copy(main_test_dir / category / fname, test_dir_main / category / fname)\n",
        "\n",
        "        print(f\"Created validation set at '{val_dir}' ({val_samples_count} samples).\")\n",
        "        print(f\"Created test set at '{test_dir_main}' (15,000 samples).\")\n",
        "    else:\n",
        "        print(\"Using existing Validation and Test sets.\")\n",
        "\n",
        "    # --- Create Training Subset Directory ---\n",
        "    # This part should run every time to create the specific subset\n",
        "    train_dir_subset = base_imdb_dir / f\"train_subset_{train_samples_count}\"\n",
        "    if os.path.exists(train_dir_subset):\n",
        "        shutil.rmtree(train_dir_subset)\n",
        "\n",
        "    os.makedirs(train_dir_subset / \"pos\")\n",
        "    os.makedirs(train_dir_subset / \"neg\")\n",
        "\n",
        "    samples_per_cat = train_samples_count // 2\n",
        "\n",
        "    for category in (\"pos\", \"neg\"):\n",
        "        train_files = os.listdir(main_train_dir / category)\n",
        "        random.shuffle(train_files)\n",
        "        for fname in train_files[:samples_per_cat]:\n",
        "            shutil.copy(main_train_dir / category / fname, train_dir_subset / category / fname)\n",
        "\n",
        "    print(f\"Created training subset at '{train_dir_subset}' ({train_samples_count} samples).\")\n",
        "    return train_dir_subset, val_dir, test_dir_main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Define Model Parameters and Text Vectorization ===\n",
        "\n",
        "# Assignment Parameters\n",
        "MAX_LENGTH = 150       # Cutoff reviews after 150 words\n",
        "MAX_TOKENS = 10000    # Consider only top 10,000 words\n",
        "INITIAL_TRAIN_SAMPLES = 100\n",
        "VAL_SAMPLES = 10000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def standardize_text(input_data):\n",
        "    \"\"\"\n",
        "    Custom standardization function to lowercase, remove punctuation, and strip HTML.\n",
        "    As suggested by lecture slides (Slide 7).\n",
        "    \"\"\"\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "def create_vectorization_layer(full_imdb_train_dir):\n",
        "    \"\"\"\n",
        "    Creates and adapts a TextVectorization layer.\n",
        "    IMPORTANT: We adapt on the *full* training set (20,000 samples)\n",
        "    to build a meaningful vocabulary, even if we only train on 100 samples.\n",
        "    \"\"\"\n",
        "    print(\"\\nAdapting TextVectorization layer on FULL training dataset...\")\n",
        "    # Use the main training directory to adapt\n",
        "    full_train_ds = keras.utils.text_dataset_from_directory(\n",
        "        full_imdb_train_dir, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    text_only_train_ds = full_train_ds.map(lambda x, y: x)\n",
        "\n",
        "    text_vectorization = layers.TextVectorization(\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=MAX_LENGTH,\n",
        "        standardize=standardize_text  # Use our custom standardization function\n",
        "    )\n",
        "    text_vectorization.adapt(text_only_train_ds)\n",
        "    print(\"Vectorization layer adapted.\")\n",
        "    return text_vectorization\n",
        "\n",
        "def vectorize_datasets(vector_layer, train_dir, val_dir, test_dir):\n",
        "    \"\"\"\n",
        "    Applies the vectorization layer to the train, val, and test datasets.\n",
        "    \"\"\"\n",
        "    train_ds = keras.utils.text_dataset_from_directory(\n",
        "        train_dir, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    val_ds = keras.utils.text_dataset_from_directory(\n",
        "        val_dir, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    test_ds = keras.utils.text_dataset_from_directory(\n",
        "        test_dir, batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    int_train_ds = train_ds.map(lambda x, y: (vector_layer(x), y), num_parallel_calls=4).prefetch(tf.data.AUTOTUNE)\n",
        "    int_val_ds = val_ds.map(lambda x, y: (vector_layer(x), y), num_parallel_calls=4).prefetch(tf.data.AUTOTUNE)\n",
        "    int_test_ds = test_ds.map(lambda x, y: (vector_layer(x), y), num_parallel_calls=4).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return int_train_ds, int_val_ds, int_test_ds"
      ],
      "metadata": {
        "id": "nzSQdGKcc7bR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === 3. Download Pre-trained Word Embeddings (GloVe) ===\n",
        "\n",
        "def get_glove_embeddings(vector_layer):\n",
        "    \"\"\"\n",
        "    Downloads GloVe embeddings and creates a pre-trained embedding matrix.\n",
        "    \"\"\"\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    print(\"\\nDownloading GloVe embeddings...\")\n",
        "    glove_zip = keras.utils.get_file(\n",
        "        \"glove.6B.zip\", url,\n",
        "        extract=True, cache_dir='.',\n",
        "        cache_subdir='') # Extracts to current dir\n",
        "\n",
        "    glove_file_1 = pathlib.Path(\"glove.6B.100d.txt\")\n",
        "    glove_file_2 = pathlib.Path(\"glove.6B\") / \"glove.6B.100d.txt\"\n",
        "\n",
        "    if glove_file_1.exists():\n",
        "        glove_file = glove_file_1\n",
        "    elif glove_file_2.exists():\n",
        "        glove_file = glove_file_2\n",
        "    else:\n",
        "        # Fallback to the 'glove_extracted' from your log\n",
        "        glove_file_3 = pathlib.Path(\"glove_extracted\") / \"glove.6B.100d.txt\"\n",
        "        if glove_file_3.exists():\n",
        "            glove_file = glove_file_3\n",
        "        else:\n",
        "            print(\"Error: Could not find 'glove.6B.100d.txt'.\")\n",
        "            print(f\"Checked: {glove_file_1}, {glove_file_2}, {glove_file_3}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"Parsing {glove_file}...\")\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "    embedding_dim = 100 # From glove.6B.100d.txt\n",
        "    vocabulary = vector_layer.get_vocabulary()\n",
        "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "    embedding_matrix = np.zeros((MAX_TOKENS, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= MAX_TOKENS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # Create the non-trainable Embedding layer\n",
        "    glove_embedding_layer = layers.Embedding(\n",
        "        MAX_TOKENS,\n",
        "        embedding_dim,\n",
        "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "        trainable=False, # Crucial! We don't train this layer.\n",
        "        mask_zero=True  # Use masking\n",
        "    )\n",
        "    print(\"GloVe embedding layer created.\")\n",
        "    return glove_embedding_layer\n"
      ],
      "metadata": {
        "id": "Qu1IpTA7dLdS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === 4. Define Model Architectures ===\n",
        "\n",
        "def build_model_scratch():\n",
        "    \"\"\"\n",
        "    Builds the RNN model with an embedding layer trained from scratch.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    # Embedding layer, 100 dimensions, trained from scratch\n",
        "    x = layers.Embedding(MAX_TOKENS, 100, mask_zero=True)(inputs)\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_model_pretrained(embedding_layer):\n",
        "    \"\"\"\n",
        "    Builds the RNN model using the pre-trained GloVe embedding layer.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    x = embedding_layer(inputs) # Use the passed pre-trained layer\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ZC7YLjBrdbsu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Training and Evaluation Function ===\n",
        "\n",
        "def train_and_evaluate(model_builder, int_train_ds, int_val_ds, int_test_ds, model_name, **builder_kwargs):\n",
        "    \"\"\"\n",
        "    Helper function to compile, train, and evaluate a model.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Training Model: {model_name} ---\")\n",
        "    # Clear session to reset model state\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    model = model_builder(**builder_kwargs)\n",
        "    # model.summary()\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\",\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        int_train_ds,\n",
        "        validation_data=int_val_ds,\n",
        "        epochs=20,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1 # Set to 1 or 2 for more detail, 0 for quiet\n",
        "    )\n",
        "\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    print(f\"Best Validation Accuracy ({model_name}): {best_val_acc:.4f}\")\n",
        "\n",
        "    # Evaluate on the final, unseen test set\n",
        "    test_loss, test_acc = model.evaluate(int_test_ds, verbose=0)\n",
        "    print(f\"Final Test Accuracy ({model_name}): {test_acc:.4f}\")\n",
        "    return best_val_acc, test_acc"
      ],
      "metadata": {
        "id": "Yx2h6kePdgs9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. Main Experiment ===\n",
        "\n",
        "def run_experiment():\n",
        "    \"\"\"\n",
        "    Executes the full assignment, comparing models and looping over sample sizes.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Part 1: Initial Setup (100 Samples) ---\n",
        "    base_imdb_dir = download_and_extract_imdb()\n",
        "    if base_imdb_dir is None:\n",
        "        print(\"Failed to setup IMDB directory. Exiting.\")\n",
        "        return\n",
        "\n",
        "    train_subset_dir_100, val_dir, test_dir = setup_custom_directories(\n",
        "        base_imdb_dir=base_imdb_dir,\n",
        "        train_samples_count=INITIAL_TRAIN_SAMPLES,\n",
        "        val_samples_count=VAL_SAMPLES\n",
        "    )\n",
        "\n",
        "    # Adapt vectorizer on main 'aclImdb/train' directory\n",
        "    vector_layer = create_vectorization_layer(base_imdb_dir / \"train\")\n",
        "\n",
        "    # Get vectorized datasets for the 100-sample experiment\n",
        "    train_ds_100, val_ds_10k, test_ds_15k = vectorize_datasets(\n",
        "        vector_layer, train_subset_dir_100, val_dir, test_dir\n",
        "    )\n",
        "\n",
        "    # Get pre-trained GloVe layer\n",
        "    glove_layer = get_glove_embeddings(vector_layer)\n",
        "    if glove_layer is None:\n",
        "        print(\"Failed to load GloVe embeddings. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- Part 2: Run Comparison for 100 Samples ---\n",
        "    print(\"\\n\\n=== EXPERIMENT 1: 100 Training Samples ===\")\n",
        "\n",
        "    val_acc_scratch_100, test_acc_scratch_100 = train_and_evaluate(\n",
        "        build_model_scratch,\n",
        "        train_ds_100, val_ds_10k, test_ds_15k,\n",
        "        \"Embedding (from Scratch)\"\n",
        "    )\n",
        "\n",
        "    val_acc_glove_100, test_acc_glove_100 = train_and_evaluate(\n",
        "        build_model_pretrained,\n",
        "        train_ds_100, val_ds_10k, test_ds_15k,\n",
        "        \"Embedding (GloVe Pre-trained)\",\n",
        "        embedding_layer=glove_layer\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- EXPERIMENT 1 RESULTS ---\")\n",
        "    print(f\"Test Accuracy (Scratch, 100 samples): {test_acc_scratch_100:.4f}\")\n",
        "    print(f\"Test Accuracy (GloVe, 100 samples):   {test_acc_glove_100:.4f}\")\n",
        "\n",
        "    if test_acc_glove_100 > test_acc_scratch_100:\n",
        "        print(\">>> RESULT: Pre-trained GloVe model performed better.\")\n",
        "    else:\n",
        "        print(\">>> RESULT: 'From Scratch' model performed better.\")\n",
        "\n",
        "\n",
        "    # --- Part 3: Find Crossover Point ---\n",
        "    print(\"\\n\\n=== EXPERIMENT 2: Finding Crossover Point ===\")\n",
        "    print(\"Testing different training sample sizes to see when 'from scratch' beats 'pre-trained'...\")\n",
        "\n",
        "    sample_counts = [100, 250, 500, 1000, 2000, 5000, 10000, 20000]\n",
        "\n",
        "    for count in sample_counts:\n",
        "        print(f\"\\n--- Testing with {count} samples ---\")\n",
        "\n",
        "        # Create the specific training subset\n",
        "        # This call will *not* delete val_set or test_set anymore\n",
        "        train_subset_dir, _, _ = setup_custom_directories(\n",
        "            base_imdb_dir=base_imdb_dir,\n",
        "            train_samples_count=count,\n",
        "            val_samples_count=VAL_SAMPLES\n",
        "        )\n",
        "\n",
        "        # Vectorize just the training set (val/test are already loaded)\n",
        "        # We reuse val_ds_10k and test_ds_15k from Part 1\n",
        "        int_train_ds, _, _ = vectorize_datasets(vector_layer, train_subset_dir, val_dir, test_dir)\n",
        "\n",
        "        # We only need validation accuracy for this comparison\n",
        "        val_acc_scratch, _ = train_and_evaluate(\n",
        "            build_model_scratch,\n",
        "            int_train_ds, val_ds_10k, test_ds_15k,\n",
        "            f\"Scratch ({count} samples)\"\n",
        "        )\n",
        "\n",
        "        val_acc_glove, _ = train_and_evaluate(\n",
        "            build_model_pretrained,\n",
        "            int_train_ds, val_ds_10k, test_ds_15k,\n",
        "            f\"GloVe ({count} samples)\",\n",
        "            embedding_layer=glove_layer\n",
        "        )\n",
        "\n",
        "        print(f\"--- Summary for {count} samples ---\")\n",
        "        print(f\"Val Acc (Scratch): {val_acc_scratch:.4f}\")\n",
        "        print(f\"Val Acc (GloVe):   {val_acc_glove:.4f}\")\n",
        "\n",
        "        if val_acc_scratch > val_acc_glove:\n",
        "            print(f\"\\n>>> CROSSOVER FOUND: At {count} samples, the 'from scratch' model\")\n",
        "            print(\"   achieved a higher validation accuracy than the pre-trained GloVe model.\")\n",
        "            break\n",
        "        elif count == sample_counts[-1]:\n",
        "            print(\"\\n>>> EXPERIMENT END: 'From scratch' model did not outperform\")\n",
        "            print(\"   pre-trained GloVe model within the tested sample range.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NyGETg88doKZ",
        "outputId": "d858bb3b-1011-4563-ca2d-719433be84e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up previous IMDB related files/directories if they exist...\n",
            "Cleanup complete.\n",
            "Downloading IMDB dataset...\n",
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 0us/step\n",
            "Renaming extracted directory from 'aclImdb_v1_extracted' to 'aclImdb'...\n",
            "Detected nested directory structure. Using 'aclImdb/aclImdb'.\n",
            "Removing aclImdb/aclImdb/train/unsup...\n",
            "\n",
            "Setting up custom data directories (Train: 100)...\n",
            "Creating new Validation and Test sets...\n",
            "Created validation set at 'aclImdb/aclImdb/val_set' (10000 samples).\n",
            "Created test set at 'aclImdb/aclImdb/test_set' (15,000 samples).\n",
            "Created training subset at 'aclImdb/aclImdb/train_subset_100' (100 samples).\n",
            "\n",
            "Adapting TextVectorization layer on FULL training dataset...\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Vectorization layer adapted.\n",
            "Found 100 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 15000 files belonging to 2 classes.\n",
            "\n",
            "Downloading GloVe embeddings...\n",
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
            "\u001b[1m862182613/862182613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 0us/step\n",
            "Parsing glove_extracted/glove.6B.100d.txt...\n",
            "Found 400000 word vectors.\n",
            "GloVe embedding layer created.\n",
            "\n",
            "\n",
            "=== EXPERIMENT 1: 100 Training Samples ===\n",
            "\n",
            "--- Training Model: Embedding (from Scratch) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.4295 - loss: 0.6957 - val_accuracy: 0.5167 - val_loss: 0.6927\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.6631 - loss: 0.6878 - val_accuracy: 0.5243 - val_loss: 0.6925\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.7747 - loss: 0.6815 - val_accuracy: 0.5290 - val_loss: 0.6923\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7s/step - accuracy: 0.7262 - loss: 0.6770 - val_accuracy: 0.5256 - val_loss: 0.6922\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.8153 - loss: 0.6715 - val_accuracy: 0.5191 - val_loss: 0.6921\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.8525 - loss: 0.6632 - val_accuracy: 0.5366 - val_loss: 0.6913\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.9418 - loss: 0.6434 - val_accuracy: 0.5251 - val_loss: 0.6907\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.9263 - loss: 0.6219 - val_accuracy: 0.5060 - val_loss: 0.6923\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.7759 - loss: 0.5954 - val_accuracy: 0.5482 - val_loss: 0.6871\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.9693 - loss: 0.5527 - val_accuracy: 0.5568 - val_loss: 0.6830\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.9234 - loss: 0.4697 - val_accuracy: 0.5149 - val_loss: 0.7428\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5s/step - accuracy: 0.8239 - loss: 0.4353 - val_accuracy: 0.5195 - val_loss: 0.8830\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8704 - loss: 0.3521 - val_accuracy: 0.5880 - val_loss: 0.7407\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.9415 - loss: 0.2699 - val_accuracy: 0.5663 - val_loss: 0.7334\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.1728 - val_accuracy: 0.5640 - val_loss: 0.9476\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.9663 - loss: 0.1650 - val_accuracy: 0.6311 - val_loss: 0.6885\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.9908 - loss: 0.0923 - val_accuracy: 0.6048 - val_loss: 0.7854\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0703 - val_accuracy: 0.5721 - val_loss: 0.9401\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.9845 - loss: 0.0969 - val_accuracy: 0.6205 - val_loss: 0.7862\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0439 - val_accuracy: 0.6119 - val_loss: 0.8879\n",
            "Best Validation Accuracy (Embedding (from Scratch)): 0.6311\n",
            "Final Test Accuracy (Embedding (from Scratch)): 0.6245\n",
            "\n",
            "--- Training Model: Embedding (GloVe Pre-trained) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7s/step - accuracy: 0.5063 - loss: 0.7004 - val_accuracy: 0.5003 - val_loss: 0.7539\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.5134 - loss: 0.7211 - val_accuracy: 0.4971 - val_loss: 0.6972\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.5863 - loss: 0.6835 - val_accuracy: 0.5072 - val_loss: 0.7068\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.5792 - loss: 0.6828 - val_accuracy: 0.5027 - val_loss: 0.6949\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.6037 - loss: 0.6663 - val_accuracy: 0.5010 - val_loss: 0.7056\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.5523 - loss: 0.6827 - val_accuracy: 0.5032 - val_loss: 0.7027\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.6030 - loss: 0.6688 - val_accuracy: 0.5035 - val_loss: 0.7003\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.5716 - loss: 0.6725 - val_accuracy: 0.5287 - val_loss: 0.6922\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.6595 - loss: 0.6420 - val_accuracy: 0.5249 - val_loss: 0.6906\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.6268 - loss: 0.6404 - val_accuracy: 0.5319 - val_loss: 0.6893\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.7312 - loss: 0.6104 - val_accuracy: 0.5324 - val_loss: 0.6889\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7s/step - accuracy: 0.7265 - loss: 0.6207 - val_accuracy: 0.5380 - val_loss: 0.6878\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4s/step - accuracy: 0.6977 - loss: 0.5926 - val_accuracy: 0.5427 - val_loss: 0.6900\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.7090 - loss: 0.6040 - val_accuracy: 0.5529 - val_loss: 0.6859\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - accuracy: 0.6790 - loss: 0.6053 - val_accuracy: 0.5635 - val_loss: 0.6829\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.7071 - loss: 0.5988 - val_accuracy: 0.5017 - val_loss: 0.7367\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5s/step - accuracy: 0.5956 - loss: 0.6490 - val_accuracy: 0.5277 - val_loss: 0.6945\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.7537 - loss: 0.5584 - val_accuracy: 0.5559 - val_loss: 0.6806\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.8129 - loss: 0.5583 - val_accuracy: 0.5496 - val_loss: 0.6866\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.7960 - loss: 0.5298 - val_accuracy: 0.5775 - val_loss: 0.6774\n",
            "Best Validation Accuracy (Embedding (GloVe Pre-trained)): 0.5775\n",
            "Final Test Accuracy (Embedding (GloVe Pre-trained)): 0.5764\n",
            "\n",
            "--- EXPERIMENT 1 RESULTS ---\n",
            "Test Accuracy (Scratch, 100 samples): 0.6245\n",
            "Test Accuracy (GloVe, 100 samples):   0.5764\n",
            ">>> RESULT: 'From Scratch' model performed better.\n",
            "\n",
            "\n",
            "=== EXPERIMENT 2: Finding Crossover Point ===\n",
            "Testing different training sample sizes to see when 'from scratch' beats 'pre-trained'...\n",
            "\n",
            "--- Testing with 100 samples ---\n",
            "\n",
            "Setting up custom data directories (Train: 100)...\n",
            "Using existing Validation and Test sets.\n",
            "Created training subset at 'aclImdb/aclImdb/train_subset_100' (100 samples).\n",
            "Found 100 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 15000 files belonging to 2 classes.\n",
            "\n",
            "--- Training Model: Scratch (100 samples) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8s/step - accuracy: 0.4825 - loss: 0.6932 - val_accuracy: 0.5010 - val_loss: 0.6932\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.6468 - loss: 0.6871 - val_accuracy: 0.5039 - val_loss: 0.6932\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.6248 - loss: 0.6831 - val_accuracy: 0.5016 - val_loss: 0.6933\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.6551 - loss: 0.6778 - val_accuracy: 0.5073 - val_loss: 0.6931\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8002 - loss: 0.6727 - val_accuracy: 0.5068 - val_loss: 0.6931\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8092 - loss: 0.6666 - val_accuracy: 0.5135 - val_loss: 0.6928\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8925 - loss: 0.6560 - val_accuracy: 0.5151 - val_loss: 0.6925\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.8553 - loss: 0.6462 - val_accuracy: 0.5189 - val_loss: 0.6926\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.9138 - loss: 0.6250 - val_accuracy: 0.5178 - val_loss: 0.6935\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6s/step - accuracy: 0.8862 - loss: 0.5934 - val_accuracy: 0.5194 - val_loss: 0.6954\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - accuracy: 0.8587 - loss: 0.5583 - val_accuracy: 0.5152 - val_loss: 0.7060\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.9284 - loss: 0.4859 - val_accuracy: 0.5329 - val_loss: 0.7095\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.8666 - loss: 0.4401 - val_accuracy: 0.5082 - val_loss: 0.8118\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8239 - loss: 0.4609 - val_accuracy: 0.6078 - val_loss: 0.6635\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.9755 - loss: 0.2541 - val_accuracy: 0.5952 - val_loss: 0.7195\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - accuracy: 0.9599 - loss: 0.2393 - val_accuracy: 0.6054 - val_loss: 0.6959\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5s/step - accuracy: 0.9845 - loss: 0.1651 - val_accuracy: 0.5970 - val_loss: 0.7674\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7s/step - accuracy: 0.9569 - loss: 0.2141 - val_accuracy: 0.5904 - val_loss: 0.6979\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.1614 - val_accuracy: 0.6059 - val_loss: 0.7161\n",
            "Best Validation Accuracy (Scratch (100 samples)): 0.6078\n",
            "Final Test Accuracy (Scratch (100 samples)): 0.5909\n",
            "\n",
            "--- Training Model: GloVe (100 samples) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7s/step - accuracy: 0.5318 - loss: 0.7438 - val_accuracy: 0.5069 - val_loss: 0.6978\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.5401 - loss: 0.6840 - val_accuracy: 0.5103 - val_loss: 0.6981\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.5789 - loss: 0.6679 - val_accuracy: 0.5051 - val_loss: 0.7142\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.6497 - loss: 0.6445 - val_accuracy: 0.5124 - val_loss: 0.7008\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.6838 - loss: 0.6121 - val_accuracy: 0.5158 - val_loss: 0.7023\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.6730 - loss: 0.6257 - val_accuracy: 0.5179 - val_loss: 0.7016\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.7441 - loss: 0.5963 - val_accuracy: 0.5386 - val_loss: 0.6904\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.7092 - loss: 0.5920 - val_accuracy: 0.5334 - val_loss: 0.6935\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - accuracy: 0.7375 - loss: 0.5740 - val_accuracy: 0.5541 - val_loss: 0.6869\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.7642 - loss: 0.5532 - val_accuracy: 0.5510 - val_loss: 0.6883\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.7818 - loss: 0.5472 - val_accuracy: 0.5074 - val_loss: 0.7671\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.6758 - loss: 0.5710 - val_accuracy: 0.5703 - val_loss: 0.6817\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.8176 - loss: 0.4970 - val_accuracy: 0.5204 - val_loss: 0.7405\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.7066 - loss: 0.5454 - val_accuracy: 0.5719 - val_loss: 0.6813\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - accuracy: 0.8047 - loss: 0.4692 - val_accuracy: 0.5584 - val_loss: 0.6932\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.8143 - loss: 0.4607 - val_accuracy: 0.5937 - val_loss: 0.6737\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5s/step - accuracy: 0.7568 - loss: 0.4627 - val_accuracy: 0.5248 - val_loss: 0.7833\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.8030 - loss: 0.4426 - val_accuracy: 0.5640 - val_loss: 0.7147\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.7988 - loss: 0.4204 - val_accuracy: 0.6109 - val_loss: 0.6660\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7s/step - accuracy: 0.8523 - loss: 0.4023 - val_accuracy: 0.5399 - val_loss: 0.7842\n",
            "Best Validation Accuracy (GloVe (100 samples)): 0.6109\n",
            "Final Test Accuracy (GloVe (100 samples)): 0.6001\n",
            "--- Summary for 100 samples ---\n",
            "Val Acc (Scratch): 0.6078\n",
            "Val Acc (GloVe):   0.6109\n",
            "\n",
            "--- Testing with 250 samples ---\n",
            "\n",
            "Setting up custom data directories (Train: 250)...\n",
            "Using existing Validation and Test sets.\n",
            "Created training subset at 'aclImdb/aclImdb/train_subset_250' (250 samples).\n",
            "Found 250 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 15000 files belonging to 2 classes.\n",
            "\n",
            "--- Training Model: Scratch (250 samples) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.4958 - loss: 0.6931 - val_accuracy: 0.5147 - val_loss: 0.6929\n",
            "Epoch 2/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.5979 - loss: 0.6895 - val_accuracy: 0.5387 - val_loss: 0.6924\n",
            "Epoch 3/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.7046 - loss: 0.6849 - val_accuracy: 0.5417 - val_loss: 0.6919\n",
            "Epoch 4/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7888 - loss: 0.6786 - val_accuracy: 0.5449 - val_loss: 0.6911\n",
            "Epoch 5/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.8558 - loss: 0.6674 - val_accuracy: 0.5532 - val_loss: 0.6895\n",
            "Epoch 6/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.8529 - loss: 0.6544 - val_accuracy: 0.5589 - val_loss: 0.6854\n",
            "Epoch 7/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8403 - loss: 0.6257 - val_accuracy: 0.5898 - val_loss: 0.6741\n",
            "Epoch 8/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.8291 - loss: 0.5539 - val_accuracy: 0.6129 - val_loss: 0.6559\n",
            "Epoch 9/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.8694 - loss: 0.4325 - val_accuracy: 0.6349 - val_loss: 0.6403\n",
            "Epoch 10/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9122 - loss: 0.3682 - val_accuracy: 0.6625 - val_loss: 0.6171\n",
            "Epoch 11/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9502 - loss: 0.2329 - val_accuracy: 0.6771 - val_loss: 0.6099\n",
            "Epoch 12/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9716 - loss: 0.1741 - val_accuracy: 0.6623 - val_loss: 0.7011\n",
            "Epoch 13/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.9736 - loss: 0.1365 - val_accuracy: 0.6726 - val_loss: 0.7101\n",
            "Epoch 14/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.1039 - val_accuracy: 0.6741 - val_loss: 0.7604\n",
            "Epoch 15/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9611 - loss: 0.1254 - val_accuracy: 0.6710 - val_loss: 0.6102\n",
            "Epoch 16/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0703 - val_accuracy: 0.6923 - val_loss: 0.6646\n",
            "Epoch 17/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0293 - val_accuracy: 0.6818 - val_loss: 0.8549\n",
            "Epoch 18/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0238 - val_accuracy: 0.6947 - val_loss: 0.7870\n",
            "Epoch 19/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.9875 - loss: 0.0397 - val_accuracy: 0.6696 - val_loss: 0.6326\n",
            "Epoch 20/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0580 - val_accuracy: 0.6938 - val_loss: 0.6917\n",
            "Best Validation Accuracy (Scratch (250 samples)): 0.6947\n",
            "Final Test Accuracy (Scratch (250 samples)): 0.6935\n",
            "\n",
            "--- Training Model: GloVe (250 samples) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.5126 - loss: 0.7165 - val_accuracy: 0.5174 - val_loss: 0.6950\n",
            "Epoch 2/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.5276 - loss: 0.6822 - val_accuracy: 0.5435 - val_loss: 0.6873\n",
            "Epoch 3/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6116 - loss: 0.6657 - val_accuracy: 0.5478 - val_loss: 0.6843\n",
            "Epoch 4/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5510 - loss: 0.6764 - val_accuracy: 0.5420 - val_loss: 0.6838\n",
            "Epoch 5/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.5943 - loss: 0.6635 - val_accuracy: 0.5692 - val_loss: 0.6777\n",
            "Epoch 6/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6884 - loss: 0.6049 - val_accuracy: 0.5720 - val_loss: 0.6743\n",
            "Epoch 7/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6816 - loss: 0.6130 - val_accuracy: 0.5700 - val_loss: 0.6742\n",
            "Epoch 8/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.6233 - loss: 0.6292 - val_accuracy: 0.5926 - val_loss: 0.6685\n",
            "Epoch 9/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.7452 - loss: 0.5866 - val_accuracy: 0.6032 - val_loss: 0.6604\n",
            "Epoch 10/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6983 - loss: 0.5822 - val_accuracy: 0.5988 - val_loss: 0.6716\n",
            "Epoch 11/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.7214 - loss: 0.5556 - val_accuracy: 0.5549 - val_loss: 0.7546\n",
            "Epoch 12/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6576 - loss: 0.5918 - val_accuracy: 0.6252 - val_loss: 0.6473\n",
            "Epoch 13/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.7525 - loss: 0.5292 - val_accuracy: 0.6237 - val_loss: 0.6492\n",
            "Epoch 14/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.7713 - loss: 0.5219 - val_accuracy: 0.6235 - val_loss: 0.6485\n",
            "Epoch 15/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.7974 - loss: 0.4870 - val_accuracy: 0.5828 - val_loss: 0.7265\n",
            "Epoch 16/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.7782 - loss: 0.4866 - val_accuracy: 0.5967 - val_loss: 0.6814\n",
            "Epoch 17/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7457 - loss: 0.4957 - val_accuracy: 0.6269 - val_loss: 0.6488\n",
            "Epoch 18/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.7971 - loss: 0.4565 - val_accuracy: 0.5521 - val_loss: 0.8381\n",
            "Epoch 19/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.7751 - loss: 0.4922 - val_accuracy: 0.6346 - val_loss: 0.6511\n",
            "Epoch 20/20\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8609 - loss: 0.3867 - val_accuracy: 0.6261 - val_loss: 0.6637\n",
            "Best Validation Accuracy (GloVe (250 samples)): 0.6346\n",
            "Final Test Accuracy (GloVe (250 samples)): 0.6233\n",
            "--- Summary for 250 samples ---\n",
            "Val Acc (Scratch): 0.6947\n",
            "Val Acc (GloVe):   0.6346\n",
            "\n",
            ">>> CROSSOVER FOUND: At 250 samples, the 'from scratch' model\n",
            "   achieved a higher validation accuracy than the pre-trained GloVe model.\n"
          ]
        }
      ]
    }
  ]
}